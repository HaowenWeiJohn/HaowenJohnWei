---
title: "IndexPen: Two-Finger Text Input with Millimeter-Wave Radar."
collection: publications
authors: '**_Wei, Haowen_**\*, Ziheng Li\*, Alexander D. Galvan, Zhuoran Su, Xiao Zhang, Kaveh Pahlavan, and Erin T. Solovey.'
permalink: /publication/2022-09-07-IndexPen
excerpt: 'In this paper, we introduce IndexPen, a novel interaction technique for text input through two-finger in-air micro-gestures, enabling touch-free, effortless, tracking-based interaction, designed to mirror real-world writing. Our system is based on millimeter-wave radar sensing, and does not require instrumentation on the user. IndexPen can successfully identify 30 distinct gestures, representing the letters A-Z, as well as Space, Backspace, Enter, and a special Activation gesture to prevent unintentional input. Additionally, we include a noise class to differentiate gesture and non-gesture noise. We present our system design, including the radio frequency (RF) processing pipeline, classification model, and real-time detection algorithms. We further demonstrate our proof-of-concept system with data collected over ten days with five participants yielding 95.89% cross-validation accuracy on 31 classes (including noise). Moreover, we explore the learnability and adaptability of our system for real-world text input with 16 participants who are first-time users to IndexPen over five sessions. After each session, the pre-trained model from the previous five-user study is calibrated on the data collected so far for a new user through transfer learning. The F-1 score showed an average increase of 9.14% per session with the calibration, reaching an average of 88.3% on the last session across the 16 users. Meanwhile, we show that the users can type sentences with IndexPen at 86.2% accuracy, measured by string similarity. This work builds a foundation and vision for future interaction interfaces that could be enabled with this paradigm.'
date: 2022-09-07
venue: '[Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 6, no. 2 (2022): 1-39.](https://dl.acm.org/doi/10.1145/3534601)'
paperurl: 'http://haowenweijohn.github.io/files/publications/2022-07-07-IndexPen.pdf'
dataseturl: 'https://www.kaggle.com/datasets/haowenjohnwei/indexpen-user-study'
videourl: 'https://www.youtube.com/watch?v=k_DA7Dgi5KY&t=8s'
---


![TeaserImage](../images/publications/2022-07-07-IndexPen-Teaser.png)

**Duration:** August 2019 â€“ September 2022  
**Role:** Project Lead, Lead Software Engineer, First Author  
**Advisor:** [Dr. Erin Solovey](https://users.wpi.edu/~esolovey/index.html)   &  [Dr. Kaveh Pahlavan](https://en.wikipedia.org/wiki/Kaveh_Pahlavan)


## Overview

**IndexPen** is a novel interaction technique for **touch-free text input** using two-finger in-air **micro-gestures**. Our system leverages **millimeter-wave radar sensing** without user instrumentation. IndexPen can recognize **30 distinct gestures**, including **A-Z, Space, Backspace, Enter**, and a special **Activation gesture**. A **noise class** differentiates between gestures and non-gestural noise. The system achieved **95.89% accuracy** across **31 classes** in a 10-day study with five participants. In a separate study, **16 first-time users** improved their performance over **five sessions** using **transfer learning**, reaching an **88.3% F-1 score** and **86.2% sentence accuracy**. This work demonstrates the potential for **future gesture-based text input interfaces**.

As part of my undergraduate project at Worcester Polytechnic Institute (WPI), I led and built **IndexPen** from scratch, creating a novel touch-free text input system using **millimeter-wave radar** to detect **two-finger in-air micro-gestures**. The system mirrors natural handwriting, enabling users to input the English alphabet without physical contact, making it ideal for hands-free and sterile environments.

## Key Contributions
- **Leadership**: Spearheaded the project from concept to completion, managing data collection, machine learning model development, and signal processing.
- **Deep Learning**: Designed and implemented a **CNN + LSTM** model for gesture recognition using radar data.
- **User Study**: Conducted user studies with **30 participants**, demonstrating high accuracy and usability.

## Novelty
- Developed a new **touch-free text input** system using radar and deep learning, offering an innovative solution for **gesture-based Human-Computer Interaction**.

## Awards & Achievements
- **Second Best Undergraduate Major Qualifying Project** at **Worcester Polytechnic Institute**.


**Supplementary Video:**

[![IndexPen](https://img.youtube.com/vi/k_DA7Dgi5KY/0.jpg)](https://www.youtube.com/watch?v=k_DA7Dgi5KY&t=8s)



