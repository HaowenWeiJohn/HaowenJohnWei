---
title: "From Brain–Computer Interfaces to AI-Enhanced Diagnostics: Developing Cutting-Edge Tools for Medical and Interactive Technologies"
collection: publications
authors: '**Haowen Wei**, Steven K. Feiner, Paul Sajda, Kaveri Thakoor'
permalink: /publication/2024-06-11-Thesis
excerpt: "My master’s thesis advances brain-computer interfaces (BCI), human-computer interaction (HCI), and extended reality (XR) through three key projects. First, PhysioLabXR, an open-source Python platform, enables real-time, multi-modal BCI and XR experiments, streamlining data processing, visualization, and machine learning. Second, our work on Interactively Assisting Glaucoma Diagnosis employs deep learning to support clinical decision-making, aiming to introduce an AI-based diagnostic tool to CHI 2025. Lastly, the In Search for an Intuitive and Efficient Text-Entry in Mixed Reality project explores innovative text-entry methods in mixed reality for enhanced user interaction. Together, these projects push the boundaries of HCI and BCI research."
date: 2018-06-11
venue: 'Columbia University'
paperurl: 'https://scholar.google.com/citations?view_op=view_citation&hl=en&user=phrai3MAAAAJ&citation_for_view=phrai3MAAAAJ:Y0pCki6q_DkC'
---

**Duration:** Sep 2022 – May 2024   
**Advisor:** [Dr. Steven K. Feiner](https://www.engineering.columbia.edu/faculty/steven-feiner) & [Dr. Paul Sajda](https://www.bme.columbia.edu/faculty/paul-sajda) & [Dr. Kaveri Thakoor](https://www.vagelos.columbia.edu/profile/kaveri-thakoor-phd)

## Overview

This master's thesis presents three innovative projects at the intersection of brain-computer interfaces (BCI), human-computer interaction (HCI), and extended reality (XR). **PhysioLabXR** is an open-source platform for real-time, multi-modal data processing in neuroscience and HCI experiments. **Interactively Assisting Glaucoma Diagnosis with an Expert Knowledge-Distilled Vision Transformer** uses deep learning to enhance clinical decision-making in glaucoma diagnosis. **In Search for an Intuitive and Efficient Text-Entry in Mixed Reality: Tap, Gaze & Pinch, SwEYEpe** explores new text-entry methods in mixed reality environments.

---

## PhysioLabXR: Real-Time, Multi-Modal Brain–Computer Interfaces and Extended Reality

PhysioLabXR is an open-source platform for real-time physiological data processing in neuroscience and HCI experiments. It supports EEG, EMG, eye trackers, fNIRS, and more, with features like multi-stream visualization and digital signal processing.

**Key Features**:
- **Real-Time Data Processing**: Visualize, record, and replay multi-modal data streams.
- **Multi-Modal Support**: Supports various sensors for complex XR and BCI experiments.
- **Extensibility**: Offers a Python scripting interface for custom data processing pipelines.

**Applications**: Used in VR, AR, and neuroscience research, PhysioLabXR fills a crucial gap by providing a robust, all-in-one platform.

**Publication**: Published in the *[Journal of Open Source Software](https://joss.theoj.org/papers/10.21105/joss.05854)*.

---

## Interactively Assisting Glaucoma Diagnosis with a Vision Transformer

This project enhances glaucoma diagnosis using an expert knowledge-distilled Vision Transformer, providing AI-augmented insights to ophthalmologists.

**Key Features**:
- **Expert Model**: Focuses on key diagnostic features in retinal images.
- **Augmented Insights**: Highlights areas of interest for improved diagnosis.
- **User Study**: Validated with 15 ophthalmologists.

**Significance**: Demonstrates how AI can support clinical decision-making, aiming for more accurate glaucoma diagnosis.

**Status**: Submitted to CHI 2025.

---

## Efficient Text-Entry in Mixed Reality: Tap, Gaze & Pinch, SwEYEpe

This project explores intuitive text-entry methods in mixed reality (MR), combining modalities like tapping, gaze, pinching, and swiping.

**Key Features**:
- **Multi-Modal Interaction**: Offers natural text-entry experiences.
- **User-Centric Design**: Focuses on usability and efficiency.
- **Evaluation**: User studies assess the effectiveness of proposed methods.

**Significance**: Aims to enhance user interaction in MR through more intuitive text-entry solutions.

**Status**: CHI 2025 Late-Breaking Work.
