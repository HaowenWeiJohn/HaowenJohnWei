---
title: "From Brain–Computer Interfaces to AI-Enhanced Diagnostics: Developing Cutting-Edge Tools for Medical and Interactive Technologies"
collection: publications
authors: '**Haowen Wei**, Steven K. Feiner, Paul Sajda, Kaveri Thakoor'
permalink: /publication/2024-06-11-Thesis
excerpt: "My master's thesis focuses on the development of innovative tools and methods in brain-computer interfaces (BCI), human-computer interaction (HCI), and extended reality (XR). It comprises three key projects. PhysioLabXR, a Python-based platform, provides a versatile environment for real-time, multi-modal BCI and XR experiments. This open-source software addresses the need for an all-in-one solution, facilitating data processing, visualization, and machine learning pipelines in various research domains. The second project, Interactively Assisting Glaucoma Diagnosis with an Expert Knowledge-distilled Vision Transformer, leverages deep learning to enhance clinical decision-making. By submitting this work to CHI 2025, we aim to introduce a novel AI-based diagnostic tool that empowers ophthalmologists with augmented diagnostic insights. Lastly, the third project, In Search for an Intuitive and Efficient Text-Entry in Mixed Reality: Tap, Gaze & Pinch, SwEYEpe, investigates innovative text-entry techniques in mixed reality environments. Targeted at CHI 2025 Late-Breaking Work, this research seeks to enhance user interaction in MR by exploring intuitive and efficient input methods. Together, these projects represent a comprehensive approach to advancing HCI and BCI research, offering new tools and methodologies for both medical and interactive technologies."
date: 2018-06-11
venue: 'Columbia University'
paperurl: 'https://scholar.google.com/citations?view_op=view_citation&hl=en&user=phrai3MAAAAJ&citation_for_view=phrai3MAAAAJ:Y0pCki6q_DkC'
---

**Duration:** Sep 2022 – May 2024   
**Advisor:** [Dr. Steven K. Feiner](https://www.engineering.columbia.edu/faculty/steven-feiner) & [Dr. Paul Sajda](https://www.bme.columbia.edu/faculty/paul-sajda) & [Dr. Kaveri Thakoor](https://www.vagelos.columbia.edu/profile/kaveri-thakoor-phd)

## Overview

This master's thesis presents three innovative projects at the intersection of brain-computer interfaces (BCI), human-computer interaction (HCI), and extended reality (XR). **PhysioLabXR** is an open-source platform for real-time, multi-modal data processing in neuroscience and HCI experiments. **Interactively Assisting Glaucoma Diagnosis with an Expert Knowledge-Distilled Vision Transformer** uses deep learning to enhance clinical decision-making in glaucoma diagnosis. **In Search for an Intuitive and Efficient Text-Entry in Mixed Reality: Tap, Gaze & Pinch, SwEYEpe** explores new text-entry methods in mixed reality environments.

---

## PhysioLabXR: Real-Time, Multi-Modal Brain–Computer Interfaces and Extended Reality

PhysioLabXR is an open-source platform for real-time physiological data processing in neuroscience and HCI experiments. It supports EEG, EMG, eye trackers, fNIRS, and more, with features like multi-stream visualization and digital signal processing.

**Key Features**:
- **Real-Time Data Processing**: Visualize, record, and replay multi-modal data streams.
- **Multi-Modal Support**: Supports various sensors for complex XR and BCI experiments.
- **Extensibility**: Offers a Python scripting interface for custom data processing pipelines.

**Applications**: Used in VR, AR, and neuroscience research, PhysioLabXR fills a crucial gap by providing a robust, all-in-one platform.

**Publication**: Published in the *Journal of Open Source Software*.

---

## Interactively Assisting Glaucoma Diagnosis with a Vision Transformer

This project enhances glaucoma diagnosis using an expert knowledge-distilled Vision Transformer, providing AI-augmented insights to ophthalmologists.

**Key Features**:
- **Expert Model**: Focuses on key diagnostic features in retinal images.
- **Augmented Insights**: Highlights areas of interest for improved diagnosis.
- **User Study**: Validated with 15 ophthalmologists.

**Significance**: Demonstrates how AI can support clinical decision-making, aiming for more accurate glaucoma diagnosis.

**Status**: Submitted to CHI 2025.

---

## Efficient Text-Entry in Mixed Reality: Tap, Gaze & Pinch, SwEYEpe

This project explores intuitive text-entry methods in mixed reality (MR), combining modalities like tapping, gaze, pinching, and swiping.

**Key Features**:
- **Multi-Modal Interaction**: Offers natural text-entry experiences.
- **User-Centric Design**: Focuses on usability and efficiency.
- **Evaluation**: User studies assess the effectiveness of proposed methods.

**Significance**: Aims to enhance user interaction in MR through more intuitive text-entry solutions.

**Status**: CHI 2025 Late-Breaking Work.
